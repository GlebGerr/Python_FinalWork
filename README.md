# Итоговое задание по дисциплине "Python для инженерии данных"

Проект представляет собой систему для автоматизированной обработки данных с использованием популярных технологий и инструментов. Все сервисы проектируются для запуска на локальной машине с минимальной настройкой.

## Стек технологий

- **Airflow**: Оркестрация процессов.
- **Spark**: Обработка данных.
- **PostgreSQL и MySQL**: Реляционные базы данных.
- **Kafka**: Брокер сообщений.
- **Python**: Генераторы данных для PostgreSQL и Kafka.
- **Jupyter Notebook**: Инструмент для работы и анализа данных.
- **Docker**: Контейнеризация всех сервисов.

## Состав системы

Система состоит из 14 контейнеров:

1. **PostgreSQL** — исходная база данных.
2. **MySQL** — целевая база данных для аналитики.
3. **Spark Master** и **Spark Worker** — обработка данных с помощью Spark.
4. **pg_datagen** — генератор данных для PostgreSQL.
5. **Kafka** — брокер сообщений.
6. **Kafka-Producer** — генератор событий, например, регистрации пользователей.
7. **Kafka-Consumer** — обработчик сообщений Kafka, добавляющий данные в PostgreSQL.
8. **Zookeeper** — управляющий кластером Kafka.
9. **Airflow** — для оркестрации всех задач и процессов.
10. **Python Dev (Jupyter)** — среда для разработки и анализа данных.

## Основные шаги

1. **Генерация данных**: Система генерирует данные о пользователях, товарах, заказах и отзывах.
2. **Репликация данных**: С помощью Airflow данные из PostgreSQL реплицируются в MySQL для аналитики.
3. **Стриминг данных**: Через Kafka передаются события, такие как регистрации пользователей и отзывы о товарах.
4. **Обработка данных**: Spark используется для трансформации данных и их последующего сохранения в MySQL.
5. **Аналитические витрины**: Витрины данных формируются для анализа поведения пользователей, рейтинга товаров и среднего чека.

## Настройка и запуск

1. Склонируйте репозиторий:

    ```bash
    git clone https://github.com/GlebGerr/Python_FinalWork.git
    cd Python_FinalWork
    ```

2. Настройте переменные окружения в файле `.env` (например, логины и пароли для сервисов).

3. Запустите проект с помощью Docker Compose:

    ```bash
    docker-compose up --build
    ```

4. После развертывания проекта интерфейсы будут доступны на следующих портах:
   - PostgreSQL: `localhost:5432`
   - MySQL: `localhost:3306`
   - Airflow Web UI: `localhost:8080`
   - Kafka: `localhost:9092`
   - Jupyter Notebook: `localhost:8888`

## Структура проекта

```
Python_FinalWork/
├── .env                      # Переменные окружения для всех сервисов
├── docker-compose.yml        # Конфигурация Docker Compose
├── docker-containers         # Конфигурация контейнеров
│   ├── airflow/              # Airflow
│   ├── db/                   # СУБД (PostgreSQL, MySQL)
│   ├── messaging/            # Kafka
│   ├── python-dev/           # Контейнер для Python и Jupyter
│   └── spark/                # Spark
└── python-scripts/           # Скрипты для обработки данных
```

## Пример работы

1. После запуска контейнеров можно проверить работу DAG в **Airflow UI**. Каждый DAG отвечает за различные части системы, такие как репликация, обработка данных или создание аналитических витрин.
2. Через **Jupyter Notebook** можно работать с данными, запускать примерные ноутбуки и выполнять аналитику.

## Структура данных

Проект включает несколько основных таблиц:

- **users** — информация о пользователях.
- **products** — информация о товарах.
- **orders** — данные о заказах.
- **reviews** — отзывы о товарах.
- **loyaltyPoints** — система лояльности.
